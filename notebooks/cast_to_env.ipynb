{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfc9a1d-1947-43b8-9baf-dc69ab78e17b",
   "metadata": {},
   "source": [
    "# Convert the fitted GPyTorch model to a TorchRL Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b547463-c093-4df6-b0ca-9e1a91e06333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57624252-d53d-455b-9b64-d3e3f9a37a07",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237da177-1db4-4005-8d86-f3546a2ad787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from tensordict import TensorDict\n",
    "import torchopt\n",
    "from torchrl.envs import ParallelEnv\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import RandomPolicy\n",
    "from torchrl.data import ReplayBuffer\n",
    "from torchrl.data import LazyTensorStorage\n",
    "\n",
    "from torch_pilco.model_learning.dynamical_models import (\n",
    "    ExactDynamicalModel,\n",
    "    ExactFit,\n",
    ")\n",
    "from torch_pilco.policy_learning.controllers import SumOfGaussians\n",
    "from torch_pilco.rewards import pendulum_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80218de1-d328-44aa-828d-d213b688a7f3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd7fb89-0441-4d55-a1f9-ce31b70229f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pendulum_training_data(\n",
    "    data_tensordict: TensorDict,\n",
    " ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    return data_tensordict['observation'].float(), data_tensordict['action'].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c1994-58de-44c1-822d-e1eb55240b0e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb6c157-065c-4d8b-9fb4-2a2faf588584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using GPU backend.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Using GPU backend.\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"MPS is available. Using MPS backend.\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b76d41-6f7b-4086-9400-5cad179851c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8ad265-c732-476e-ac98-544a4d530573",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_batch = 100\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")\n",
    "random_policy = RandomPolicy(env.action_spec)\n",
    "action_dim = env.action_space.shape[0]\n",
    "x = env.reset()\n",
    "state_dim = x['observation'].shape[0]\n",
    "\n",
    "num_particles = 400\n",
    "num_basis = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956420e9-43f9-4841-a41f-3d8df9e952b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_policy = SumOfGaussians(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    num_basis,\n",
    "    u_max=env.action_space.high[0],\n",
    "    dtype=torch.float32,\n",
    ") \n",
    "batched_policy = torch.vmap(control_policy, in_dims=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c542d0f-3791-42a3-b09d-3392c1eafb36",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2388a92-0d21-4a09-b7cf-d34e1084356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random trajectory from the environment\n",
    "# Should create about 5-8 trajectories then stitch them together\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy=random_policy,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=frames_per_batch,\n",
    ")\n",
    "# Now determine how many frames are stacked for the dynamical model input:\n",
    "\n",
    "replay_buffer = ReplayBuffer(storage=LazyTensorStorage(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df01de0b-3a15-4d26-82e6-3ce2c557c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the data into the replay buffer\n",
    "# Now grab some data and fit the GP\n",
    "for data in collector:\n",
    "    # convert the tensordict from collector to a version\n",
    "    # suitable for dynamical model\n",
    "    replay_buffer.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389147de-0f3a-4537-be7d-e8db70520324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the whole buffer for data\n",
    "states, actions = build_pendulum_training_data(replay_buffer.sample(len(replay_buffer)))\n",
    "\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "    num_tasks=states.shape[1]\n",
    ")\n",
    "model = ExactDynamicalModel(\n",
    "    states,\n",
    "    actions,\n",
    "    likelihood,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0565e46-14a5-4a30-bf5f-996ddd642456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0851, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(2.8226, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(2.1481, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(2.0817, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.9098, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.8592, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.7841, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.7586, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.7402, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.7283, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6899, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6800, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6778, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6764, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6752, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6739, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6735, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6734, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6734, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6733, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6732, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6731, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6731, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6731, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6736, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.9323, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6731, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6752, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6730, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6729, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6728, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6727, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6727, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6726, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6726, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6726, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6726, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(1.6725, dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "ExactFit(model.to(torch.float64), print_loss=False, n_training_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e950392-14e2-4aed-b667-b8db82959c48",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 'reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreward\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/torch-pilco/.venv/lib/python3.12/site-packages/torchrl/envs/common.py:3981\u001b[39m, in \u001b[36m_EnvWrapper.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m   3979\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_env\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dir__\u001b[39m():\n\u001b[32m   3980\u001b[39m     env = \u001b[38;5;28mself\u001b[39m.\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m_env\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3981\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattr__\u001b[39m(attr)\n\u001b[32m   3984\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   3985\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe env wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt set in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, cannot access \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3986\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'TimeLimit' object has no attribute 'reward'"
     ]
    }
   ],
   "source": [
    "env.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c5fec-e2ae-409d-8138-a6648842d78a",
   "metadata": {},
   "source": [
    "## Convert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6e26bbf-2a13-4c9a-b3cf-9974d606fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import check_env_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efa3f6b-ed03-41a8-8357-0934d472e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_pilco.policy_learning.rollout import GPyTorchEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e8450c-31a9-4af9-bdbe-e44a1dbb485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming you have a fitted GPyTorch model named 'fitted_gp_model'):\n",
    "gp_env = GPyTorchEnv(model, env, pendulum_cost, replay_buffer, device=device, batch_size=(num_particles,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b291a96c-a6a5-41b7-811f-96aa4059b402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2025-12-18 14:13:21,440 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(gp_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7729a245-78ac-410f-b7fc-3b62fda63da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictModule\n",
    "policy = TensorDictModule(\n",
    "    batched_policy,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6fc943-95c7-43ee-a60c-f118bb5b4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward:  232.9128, last reward:  6.9028, gradient norm:  1.516: 100%|████████████████████████████████████████████████████| 5/5 [01:12<00:00, 14.59s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = num_particles\n",
    "N = 2_000\n",
    "pbar = tqdm.tqdm(range(N // batch_size))\n",
    "optim = torch.optim.Adam(control_policy.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, N)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    rollout = gp_env.rollout(35, policy)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean(dim=0).sum()\n",
    "    traj_return.backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(control_policy.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean(dim=0).item())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "399629ed-2104-49a5-9835-72b862c4fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2025-12-18 14:26:16,543 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make the batch version of our gym environment\n",
    "def make_env():\n",
    "    return GymEnv(\"Pendulum-v1\")\n",
    "check_env_specs(env)\n",
    "penv = ParallelEnv(1, make_env)\n",
    "check_env_specs(penv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b69b0ae-b043-4167-8bb1-aa4838d880a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    penv,\n",
    "    policy=policy,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=frames_per_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aa07026-9c5e-44d1-b15b-9ebf3927bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now grab some data and fit the GP\n",
    "for data in collector:\n",
    "    # convert the tensordict from collector to a version\n",
    "    # suitable for dynamical model\n",
    "    replay_buffer.extend(data)\n",
    "    # Now train with all of the data seen so far:\n",
    "    # We get this by sampling from the replay buffer as many items as there are!\n",
    "    states, actions = build_pendulum_training_data(replay_buffer.sample(len(replay_buffer)))\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "        num_tasks=states.shape[1]\n",
    "    )\n",
    "    model = ExactDynamicalModel(\n",
    "        states,\n",
    "        actions,\n",
    "        likelihood,\n",
    "    )\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    ExactFit(model, likelihood, print_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51e500b5-bc2c-4cd8-a622-482ed0661e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_env = GPyTorchEnv(model,env,pendulum_cost,replay_buffer,batch_size=(num_particles,))\n",
    "batched_policy = torch.vmap(control_policy, in_dims=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40300dd2-4290-4096-8352-d767a1e77a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TensorDictModule(\n",
    "    batched_policy,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "optim = torch.optim.Adam(control_policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65507913-6d82-44ae-b5d0-57940057eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward:  230.8026, last reward:  6.8221, gradient norm:  1.609: 100%|████████████████████████████████████████████████████| 5/5 [01:12<00:00, 14.60s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 2_000\n",
    "pbar = tqdm.tqdm(range(N // batch_size))        # unsqueeze states\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, N)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    rollout = gp_env.rollout(35, control_policy)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean(dim=0).sum()\n",
    "    traj_return.backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(control_policy.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean(dim=0).item())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa299f6-dbf1-40e0-b953-fc7c0214e411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
